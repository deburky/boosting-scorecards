{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "\n",
    "mo.md(\"\"\"\n",
    "# Categorical Encoders\n",
    "\n",
    "In this notebook we explore different ways to encode categorical data to use in classification tasks.\n",
    "\n",
    "Author: https://www.github.com/deburky\n",
    "\n",
    "Built with **marimo** üåäüçÉ\n",
    "\"\"\")# .callout(kind=\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from itertools import zip_longest\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import logit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "from category_encoders import CatBoostEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "## Creating online lending data\n",
    "\n",
    "This is a simulation of real-world portfolio of online lending.\n",
    "\n",
    "We use three categorical variables: type of account, subscription type, and bureau score to predict default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset controls\n",
    "sample_size = mo.ui.slider(500, 50_000, value=25_000, step=500, label=\"Sample size\")\n",
    "random_seed = mo.ui.number(start=1, stop=999, value=42, label=\"Random seed\")\n",
    "\n",
    "mo.vstack([\n",
    "    mo.md(\"**Dataset Configuration**\"),\n",
    "    mo.hstack([sample_size, random_seed])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed.value)\n",
    "\n",
    "\n",
    "def conditional_prob_cat_given_event(feature_dist, event_rate, overall_event_rate):\n",
    "    \"\"\"\n",
    "    Calculate P(cat | is_default=1) and P(cat | is_default=0)\n",
    "    feature_dist: dict, P(cat)\n",
    "    event_rate: dict, P(is_default=1 | cat)\n",
    "    overall_event_rate: float, P(is_default=1)\n",
    "    Returns: dicts for class 1 and class 0\n",
    "    \"\"\"\n",
    "    # P(cat | is_default=1) ‚àù P(is_default=1 | cat) * P(cat)\n",
    "    prob_given_1 = {}\n",
    "    for cat in feature_dist:\n",
    "        prob = event_rate[cat] * feature_dist[cat]\n",
    "        prob_given_1[cat] = prob\n",
    "    # Normalize\n",
    "    total_1 = sum(prob_given_1.values())\n",
    "    for cat in prob_given_1:\n",
    "        prob_given_1[cat] /= total_1\n",
    "\n",
    "    # P(cat | is_default=0) ‚àù (1 - P(is_default=1 | cat)) * P(cat)\n",
    "    prob_given_0 = {}\n",
    "    for cat in feature_dist:\n",
    "        prob = (1 - event_rate[cat]) * feature_dist[cat]\n",
    "        prob_given_0[cat] = prob\n",
    "    total_0 = sum(prob_given_0.values())\n",
    "    for cat in prob_given_0:\n",
    "        prob_given_0[cat] /= total_0\n",
    "\n",
    "    return prob_given_1, prob_given_0\n",
    "\n",
    "def generate_stratified_synthetic_data(n_samples: int = 10_000):\n",
    "    # Feature distributions (from your input)\n",
    "    is_business_dist = {False: 0.894687, True: 0.105313}\n",
    "    subscription_dist = {'BASIC': 0.895823, 'ENHANCED': 0.097411, 'CUSTOM': 0.006766}\n",
    "    bureau_rating_raw = {\n",
    "        'A': 0.084943, 'B': 0.145165, 'C': 0.160290, 'D': 0.196071,\n",
    "        'E': (0.189412 + 0.142590 + 0.081423),\n",
    "    }\n",
    "    total = sum(bureau_rating_raw.values())\n",
    "    bureau_rating_dist = {k: v / total for k, v in bureau_rating_raw.items()}\n",
    "    event_rates = {\n",
    "        'is_business': {False: 0.0384, True: 0.0621},\n",
    "        'subscription': {'BASIC': 0.0363, 'ENHANCED': 0.0797, 'CUSTOM': 0.0957},\n",
    "        'bureau_rating': {\n",
    "            'A': 0.0134, 'B': 0.0224, 'C': 0.0332, 'D': 0.0417,\n",
    "            'E': (0.0480 + 0.0601 + 0.0833),\n",
    "        }\n",
    "    }\n",
    "    # 1. Calculate expected overall default rate\n",
    "    overall_default_rate = (\n",
    "        sum([is_business_dist[k] * event_rates['is_business'][k] for k in is_business_dist]) +\n",
    "        sum([subscription_dist[k] * event_rates['subscription'][k] for k in subscription_dist]) +\n",
    "        sum([bureau_rating_dist[k] * event_rates['bureau_rating'][k] for k in bureau_rating_dist])\n",
    "    ) / 3\n",
    "\n",
    "    n_pos = int(round(n_samples * overall_default_rate))\n",
    "    n_neg = n_samples - n_pos\n",
    "\n",
    "    # 2. Get conditional distributions P(cat | y)\n",
    "    p_is_business_1, p_is_business_0 = conditional_prob_cat_given_event(is_business_dist, event_rates['is_business'], overall_default_rate)\n",
    "    p_subscription_1, p_subscription_0 = conditional_prob_cat_given_event(subscription_dist, event_rates['subscription'], overall_default_rate)\n",
    "    p_bureau_1, p_bureau_0 = conditional_prob_cat_given_event(bureau_rating_dist, event_rates['bureau_rating'], overall_default_rate)\n",
    "\n",
    "    # 3. Simulate positives and negatives\n",
    "    def draw_feature(dist, size):\n",
    "        # Draw with replacement according to distribution\n",
    "        return np.random.choice(list(dist.keys()), size=size, p=list(dist.values()))\n",
    "\n",
    "    data_pos = {\n",
    "        'user_id': [str(uuid.uuid4()) for _ in range(n_pos)],\n",
    "        'is_default': np.ones(n_pos, dtype=int),\n",
    "        'is_business': draw_feature(p_is_business_1, n_pos),\n",
    "        'subscription': draw_feature(p_subscription_1, n_pos),\n",
    "        'bureau_rating': draw_feature(p_bureau_1, n_pos)\n",
    "    }\n",
    "    data_neg = {\n",
    "        'user_id': [str(uuid.uuid4()) for _ in range(n_neg)],\n",
    "        'is_default': np.zeros(n_neg, dtype=int),\n",
    "        'is_business': draw_feature(p_is_business_0, n_neg),\n",
    "        'subscription': draw_feature(p_subscription_0, n_neg),\n",
    "        'bureau_rating': draw_feature(p_bureau_0, n_neg)\n",
    "    }\n",
    "    df = pd.concat([pd.DataFrame(data_pos), pd.DataFrame(data_neg)], ignore_index=True)\n",
    "    # Shuffle rows\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = generate_stratified_synthetic_data(sample_size.value)\n",
    "print(df.groupby('is_business')['is_default'].mean())\n",
    "print(df.groupby('subscription')['is_default'].mean())\n",
    "print(df.groupby('bureau_rating')['is_default'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "## Categorical encoders\n",
    "\n",
    "In this example, we use test different schemes of encoding categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TargetEncoder(random_state=random_seed.value, cv=2, smooth=1e1).set_output(transform=\"pandas\")\n",
    "\n",
    "features = [\"is_business\", \"subscription\", \"bureau_rating\"]\n",
    "label = \"is_default\"\n",
    "\n",
    "df_reindexed = df.sample(frac=1, random_state=random_seed.value).reset_index(drop=True)\n",
    "X, y = df_reindexed[features].copy(), df_reindexed[label].copy()\n",
    "\n",
    "ix_train, ix_test = train_test_split(X.index, stratify=y, random_state=random_seed.value)\n",
    "\n",
    "X_train, X_test = X.loc[ix_train], X.loc[ix_test]\n",
    "y_train, y_test = y.loc[ix_train], y.loc[ix_test]\n",
    "\n",
    "X_train_te = te.fit_transform(X_train, y_train)\n",
    "X_test_te = te.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_te, columns=te.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = te.get_feature_names_out()\n",
    "\n",
    "feature_to_show = mo.ui.dropdown(\n",
    "    options=feature_names,\n",
    "    value=feature_names[0],\n",
    "    label=\"Feature to display\"\n",
    ")\n",
    "feature_to_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Target encodings ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "categories, encodings = te.categories_, te.encodings_\n",
    "\n",
    "rows = []\n",
    "for fname, cats, encs in zip(feature_names, categories, encodings):\n",
    "    for cat, enc in zip_longest(cats, encs, fillvalue=pd.NA):\n",
    "        rows.append({'feature': fname, 'category': cat, 'encoding': enc})\n",
    "\n",
    "df_summary = pd.DataFrame(rows)\n",
    "df_summary['average'] = te.target_mean_\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Feature selection ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "df_feat = df_summary[df_summary['feature'] == feature_to_show.value].copy()\n",
    "baseline = df_feat['average'].iloc[0]\n",
    "\n",
    "# The x-axis labels\n",
    "df_feat['label'] = df_feat['category'].astype(str)\n",
    "\n",
    "# Bar or line for cumulative probability per category\n",
    "line1 = alt.Chart(df_feat).mark_bar(color='teal').encode(\n",
    "    x=alt.X('label:N', title='Category'),\n",
    "    y=alt.Y('encoding:Q', title='Default Rate / Probability of Default', axis=alt.Axis(format='%'))\n",
    ")\n",
    "\n",
    "# Horizontal dashed average line\n",
    "avg_line = alt.Chart(pd.DataFrame({'rate': [baseline], 'desc': ['Average']})).mark_rule(\n",
    "    color='orange', strokeDash=[10,10], size=3\n",
    ").encode(\n",
    "    y='rate:Q',\n",
    "    tooltip=[alt.Tooltip('rate:Q', format='.2%'), alt.Tooltip('desc:N')]\n",
    ").properties(title=feature_to_show.value)\n",
    "avg_text = alt.Chart(pd.DataFrame({'rate': [baseline]})).mark_text(\n",
    "    align='left', baseline='bottom', dx=5, dy=-5, color='black'\n",
    ").encode(\n",
    "    x=alt.value(0),\n",
    "    y='rate:Q',\n",
    "    text=alt.value('Average')\n",
    ")\n",
    "\n",
    "chart = (line1 + avg_line + avg_text).properties(width=500, height=350)\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "## From probabilities to WOE\n",
    "\n",
    "Now each categorical group has a value corresponding to the the average event rate in that category. To make this data transformation more interpretable, we can convert these values to Weight of Evidence (WOE) scores by using a trick. This formula is due to A. Turing / J. Good.\n",
    "\n",
    "If we center the encoded probabilities the average event rate of the sample, we can create a form of standardized scores that measure how far each group lies from the average.\n",
    "\n",
    "Since most models will produce an average score which is close to the true average (unless class weights or other algorithm-level adjustment is used), we can use either observed rate or expected (average of predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WOEScorer:\n",
    "    \"\"\"Fit any encoder per feature and convert encodings to WOE directly.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, random_state=42, encoder_class=TargetEncoder, encoder_kwargs=None\n",
    "    ):\n",
    "        self.random_state = random_state\n",
    "        self.encoder_class = encoder_class\n",
    "        self.encoder_kwargs = encoder_kwargs or {}\n",
    "        self.encoders = {}\n",
    "        self.mapping_ = {}\n",
    "        self.y_prior_ = None\n",
    "        self.is_fitted_ = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        self.y_prior_ = y.mean()\n",
    "        odds_prior = self.y_prior_ / (1 - self.y_prior_)\n",
    "\n",
    "        for col in X.columns:\n",
    "            enc_kwargs = self.encoder_kwargs.copy()\n",
    "            enc_kwargs[\"random_state\"] = self.random_state\n",
    "            # Remove unused kwargs for CatBoostEncoder\n",
    "            if self.encoder_class.__name__ == \"CatBoostEncoder\":\n",
    "                enc_kwargs.pop(\"cv\", None)\n",
    "                enc_kwargs.pop(\"smooth\", None)\n",
    "                enc = self.encoder_class(cols=[col], return_df=True, **enc_kwargs)\n",
    "                enc.fit(X[[col]], y)\n",
    "                self.encoders[col] = enc\n",
    "                # CatBoostEncoder does not expose mapping directly\n",
    "                cats = pd.Series(X[col].unique(), name=col)\n",
    "                event_rate = enc.transform(cats.to_frame()).values.flatten()\n",
    "            else:\n",
    "                enc = self.encoder_class(**enc_kwargs)\n",
    "                enc.fit(X[[col]], y)\n",
    "                self.encoders[col] = enc\n",
    "                cats = enc.categories_[0]\n",
    "                event_rate = enc.encodings_[0]\n",
    "\n",
    "            # Defensive clipping for WoE\n",
    "            event_rate = np.clip(event_rate, 1e-15, 1 - 1e-15)\n",
    "            odds_cat = event_rate / (1 - event_rate)\n",
    "            woe = np.log(odds_cat / odds_prior)\n",
    "\n",
    "            mapping = pd.DataFrame(\n",
    "                {\"category\": cats, \"event_rate\": event_rate, \"woe\": woe}\n",
    "            ).set_index(\"category\")\n",
    "            self.mapping_[col] = mapping\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        odds_prior = self.y_prior_ / (1 - self.y_prior_)\n",
    "        woe_df = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            enc = self.encoders[col]\n",
    "            event_rate = enc.transform(X[[col]])\n",
    "            # CategoryEncoders always returns a DataFrame; sklearn can return np.ndarray\n",
    "            if isinstance(event_rate, pd.DataFrame):\n",
    "                event_rate = event_rate.values.flatten()\n",
    "            event_rate = np.clip(event_rate, 1e-15, 1 - 1e-15)\n",
    "            odds_cat = event_rate / (1 - event_rate)\n",
    "            woe = np.log(odds_cat / odds_prior)\n",
    "            woe_df[f\"{col}_woe\"] = woe\n",
    "        return woe_df\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def get_mapping(self, col: str) -> pd.DataFrame:\n",
    "        return self.mapping_[col].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = WOEScorer(random_state=random_seed.value)\n",
    "X_woe_train = scorer.fit_transform(X_train, y_train)\n",
    "mo.plain(scorer.mapping_[feature_to_show.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = feature_to_show.value\n",
    "\n",
    "df_feat_woe = scorer.mapping_[feature_name].reset_index().copy()\n",
    "df_feat_woe['label'] = df_feat_woe['category'].astype(str)\n",
    "\n",
    "woe_bar = alt.Chart(df_feat_woe).mark_bar(color='teal').encode(\n",
    "    x=alt.X('label:N', title='Category'),\n",
    "    y=alt.Y('woe:Q', title='Weight of Evidence (WOE)')\n",
    ")\n",
    "woe_zero_line = alt.Chart(pd.DataFrame({'woe': [0]})).mark_rule(\n",
    "    color='orange', strokeDash=[10,10], size=3\n",
    ").encode(\n",
    "    y='woe:Q'\n",
    ")\n",
    "woe_zero_text = alt.Chart(pd.DataFrame({'woe': [0]})).mark_text(\n",
    "    align='left', baseline='bottom', dx=5, dy=-5, color='orange', fontWeight='bold'\n",
    ").encode(\n",
    "    x=alt.value(0),\n",
    "    y='woe:Q',\n",
    "    text=alt.value('WOE=0 (avg risk)')\n",
    ")\n",
    "\n",
    "chart_woe = (woe_bar + woe_zero_line + woe_zero_text).properties(\n",
    "    width=500, height=350, title=f\"WOE for {feature_name}\"\n",
    ")\n",
    "chart_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {},
   "source": [
    "## Measure performance\n",
    "\n",
    "One way to turn the transformed input into a classifier is to sum up the WOE scores for each feature and add an intercept to it. It sounds a lot like what gradient boosted trees do, but it can also be seen as logistic regression with coefficients being equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI for model hyperparameters\n",
    "smooth = mo.ui.slider(0, 1.0, value=0.5, step=0.05, label=\"Smoothing\")\n",
    "a = mo.ui.slider(0, 10, value=0, step=1, label=\"A\")\n",
    "l1_ratio = mo.ui.slider(0.0, 1.0, value=1.0, step=0.1, label=\"L1 ratio\")\n",
    "max_iter = mo.ui.slider(0, 200, value=5, step=1, label=\"Maximum iterations\")\n",
    "\n",
    "mo.vstack([\n",
    "    mo.md(\"### TargetEncoder Parameters\"),\n",
    "    mo.vstack([smooth]),\n",
    "    mo.md(\"### CatBoostEncoder Parameters\"),\n",
    "    mo.vstack([a]),\n",
    "    mo.md(\"### Logistic Regression Parameters\"),\n",
    "    mo.vstack([l1_ratio, max_iter]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Encoders and LR Parameters ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "lr_params = dict(\n",
    "    solver=\"saga\",\n",
    "    penalty=\"elasticnet\",\n",
    "    l1_ratio=l1_ratio.value,\n",
    "    max_iter=max_iter.value,\n",
    "    random_state=random_seed.value,\n",
    ")\n",
    "\n",
    "woe_te_params = dict(\n",
    "    smooth=smooth.value,\n",
    "    random_state=random_seed.value,\n",
    ")\n",
    "woe_cb_params = dict(\n",
    "    a=a.value,\n",
    ")\n",
    "\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ WOE Scorer - TargetEncoder ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "woe_scorer_te = WOEScorer(\n",
    "    random_state=random_seed.value,\n",
    "    encoder_class=TargetEncoder,\n",
    "    encoder_kwargs=woe_te_params\n",
    ")\n",
    "X_woe_te_train = woe_scorer_te.fit_transform(X_train, y_train)\n",
    "X_woe_te_test = woe_scorer_te.transform(X_test)\n",
    "\n",
    "# Scorecard-style: Sum WOE and add logit(prior)\n",
    "woe_to_p = sigmoid(X_woe_te_test.sum(axis=1) + logit(woe_scorer_te.y_prior_))\n",
    "gini_woe_te = roc_auc_score(y_test, woe_to_p) * 2 - 1\n",
    "log_loss_woe_te = log_loss(y_test, woe_to_p)\n",
    "brier_woe_te = brier_score_loss(y_test, woe_to_p)\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ WOE Scorer - CatBoostEncoder ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "woe_scorer_cb = WOEScorer(\n",
    "    random_state=random_seed.value,\n",
    "    encoder_class=CatBoostEncoder,\n",
    "    encoder_kwargs=woe_cb_params,\n",
    ")\n",
    "X_woe_cb_train = woe_scorer_cb.fit_transform(X_train, y_train)\n",
    "X_woe_cb_test = woe_scorer_cb.transform(X_test)\n",
    "\n",
    "woe_cb_to_p = sigmoid(X_woe_cb_test.sum(axis=1) + logit(woe_scorer_cb.y_prior_))\n",
    "gini_woe_cb = roc_auc_score(y_test, woe_cb_to_p) * 2 - 1\n",
    "log_loss_woe_cb = log_loss(y_test, woe_cb_to_p)\n",
    "brier_woe_cb = brier_score_loss(y_test, woe_cb_to_p)\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ WOE TargetEncoder + Logistic Regression ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "lr_model_te = LogisticRegression(**lr_params)\n",
    "lr_model_te.fit(X_woe_te_train, y_train)\n",
    "lr_to_p_te = lr_model_te.predict_proba(X_woe_te_test)[:, 1]\n",
    "gini_lr_te = roc_auc_score(y_test, lr_to_p_te) * 2 - 1\n",
    "log_loss_lr_te = log_loss(y_test, lr_to_p_te)\n",
    "brier_lr_te = brier_score_loss(y_test, lr_to_p_te)\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ WOE CatBoostEncoder + Logistic Regression ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "lr_model_cb = LogisticRegression(\n",
    "    **lr_params\n",
    ")\n",
    "lr_model_cb.fit(X_woe_cb_train, y_train)\n",
    "lr_to_p_cb = lr_model_cb.predict_proba(X_woe_cb_test)[:, 1]\n",
    "gini_lr_cb = roc_auc_score(y_test, lr_to_p_cb) * 2 - 1\n",
    "log_loss_lr_cb = log_loss(y_test, lr_to_p_cb)\n",
    "brier_lr_cb = brier_score_loss(y_test, lr_to_p_cb)\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ OneHotEncoder + Logistic Regression ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "one_hot_encoder = OneHotEncoder(drop=\"first\", sparse_output=False).set_output(\n",
    "    transform=\"pandas\"\n",
    ")\n",
    "X_ohe_train = one_hot_encoder.fit_transform(X_train)\n",
    "X_ohe_test = one_hot_encoder.transform(X_test)\n",
    "lr_ohe_model = LogisticRegression(**lr_params)\n",
    "lr_ohe_model.fit(X_ohe_train, y_train)\n",
    "lr_ohe_to_p = lr_ohe_model.predict_proba(X_ohe_test)[:, 1]\n",
    "gini_lr_ohe = roc_auc_score(y_test, lr_ohe_to_p) * 2 - 1\n",
    "log_loss_lr_ohe = log_loss(y_test, lr_ohe_to_p)\n",
    "brier_lr_ohe = brier_score_loss(y_test, lr_ohe_to_p)\n",
    "\n",
    "def nsqrt(p):\n",
    "    \"\"\"Standard error for a Bernoulli variable with mean p.\"\"\"\n",
    "    return np.sqrt(p * (1 - p))\n",
    "\n",
    "mean_woe_te = woe_to_p.mean() / 100\n",
    "stderr_woe_te = nsqrt(mean_woe_te)\n",
    "mean_woe_cb = woe_cb_to_p.mean() / 100\n",
    "stderr_woe_cb = nsqrt(mean_woe_cb)\n",
    "\n",
    "# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Display Results ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "mo.md(\n",
    "    f\"\"\"\n",
    "| Model                                      | Gini    | Log Loss  | Brier  |\n",
    "|---------------------------------------------|---------|-----------|--------|\n",
    "| WOE TargetEncoder (sum, scorecard)         | {gini_woe_te:.4f} | {log_loss_woe_te:.4f} | {brier_woe_te:.4f} |\n",
    "| WOE CatBoostEncoder (sum, scorecard)       | {gini_woe_cb:.4f} | {log_loss_woe_cb:.4f} | {brier_woe_cb:.4f} |\n",
    "| WOE TargetEncoder + Logistic Regression    | {gini_lr_te:.4f}  | {log_loss_lr_te:.4f}  | {brier_lr_te:.4f}  |\n",
    "| WOE CatBoostEncoder + Logistic Regression  | {gini_lr_cb:.4f}  | {log_loss_lr_cb:.4f}  | {brier_lr_cb:.4f}  |\n",
    "| OHE + Logistic Regression                  | {gini_lr_ohe:.4f} | {log_loss_lr_ohe:.4f} | {brier_lr_ohe:.4f} |\n",
    "|---------------------------------------------|---------|-----------|--------|\n",
    "| Log Avg P | Log STD (WOE TE) {mean_woe_te:.10f}  {stderr_woe_te:.10f} |\n",
    "| Log Avg P | Log STD (WOE CB) {mean_woe_cb:.10f} {stderr_woe_cb:.10f} |\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Create interactive controls\n",
    "feature_options = list(X_test.columns)\n",
    "encoder_options = ['TargetEncoder', 'CatBoostEncoder']\n",
    "\n",
    "feature1_slider = mo.ui.dropdown(\n",
    "    options=feature_options, \n",
    "    value=feature_options[0], \n",
    "    label=\"Feature 1 (X-axis)\"\n",
    ")\n",
    "\n",
    "feature2_slider = mo.ui.dropdown(\n",
    "    options=feature_options, \n",
    "    value=feature_options[1] if len(feature_options) > 1 else feature_options[0], \n",
    "    label=\"Feature 2 (Y-axis)\"\n",
    ")\n",
    "\n",
    "encoder_slider = mo.ui.dropdown(\n",
    "    options=encoder_options, \n",
    "    value='TargetEncoder', \n",
    "    label=\"Encoder\"\n",
    ")\n",
    "\n",
    "mo.hstack([feature1_slider, feature2_slider, encoder_slider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selected features and encoder\n",
    "feat1 = feature1_slider.value\n",
    "feat2 = feature2_slider.value\n",
    "encoder = encoder_slider.value\n",
    "\n",
    "# Choose the right WOE data and scorer based on encoder selection\n",
    "if encoder == \"TargetEncoder\":\n",
    "    woe_data = X_woe_te_test.copy()\n",
    "    scorer_ = woe_scorer_te\n",
    "else:\n",
    "    woe_data = X_woe_cb_test.copy()\n",
    "    scorer_ = woe_scorer_cb\n",
    "\n",
    "# Get WOE values for selected features\n",
    "feat1_woe = f\"{feat1}_woe\"\n",
    "feat2_woe = f\"{feat2}_woe\"\n",
    "\n",
    "# Calculate predicted probability for each point\n",
    "woe_sum = woe_data[feat1_woe] + woe_data[feat2_woe]\n",
    "predicted_prob = sigmoid(woe_sum + logit(scorer_.y_prior_))\n",
    "\n",
    "# Create plot data\n",
    "plot_data = pd.DataFrame({\n",
    "    \"feat1_woe\": woe_data[feat1_woe],\n",
    "    \"feat2_woe\": woe_data[feat2_woe],\n",
    "    \"feat1_cat\": X_test[feat1].values,\n",
    "    \"feat2_cat\": X_test[feat2].values,\n",
    "    \"predicted_prob\": predicted_prob,\n",
    "})\n",
    "\n",
    "# Create a combined label for both categories\n",
    "plot_data[\"combined_label\"] = plot_data[\"feat1_cat\"].astype(str) + \", \" + plot_data[\"feat2_cat\"].astype(str)\n",
    "\n",
    "# For better positioning, get unique combinations only\n",
    "unique_plot_data = plot_data[\n",
    "    [\"feat1_woe\", \"feat2_woe\", \"feat1_cat\", \"feat2_cat\", \"predicted_prob\", \"combined_label\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "# Create decision surface with contour\n",
    "x_range = np.linspace(plot_data[\"feat1_woe\"].min(), plot_data[\"feat1_woe\"].max(), 100)\n",
    "y_range = np.linspace(plot_data[\"feat2_woe\"].min(), plot_data[\"feat2_woe\"].max(), 100)\n",
    "xx, yy = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Calculate probability surface\n",
    "woe_surface = xx + yy\n",
    "prob_surface = woe_surface + logit(scorer_.y_prior_)\n",
    "\n",
    "# Create contour data\n",
    "contour_data = []\n",
    "for i in range(len(x_range)):\n",
    "    for j in range(len(y_range)):\n",
    "        contour_data.append({\"feat1_woe\": xx[j, i], \"feat2_woe\": yy[j, i], \"prob\": prob_surface[j, i]})\n",
    "contour_df = pd.DataFrame(contour_data)\n",
    "\n",
    "# Contour plot (decision surface)\n",
    "contour = (\n",
    "    alt.Chart(contour_df)\n",
    "    .mark_rect(opacity=0.3)\n",
    "    .encode(\n",
    "        x=alt.X(\"feat1_woe:Q\", title=f\"{feat1} WOE\"),\n",
    "        y=alt.Y(\"feat2_woe:Q\", title=f\"{feat2} WOE\"),\n",
    "        color=alt.Color(\"prob:Q\", scale=alt.Scale(scheme=\"purples\", domain=[-2, 2]), title=\"Predicted Log Odds\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Scatter plot with actual data points\n",
    "scatter = (\n",
    "    alt.Chart(unique_plot_data)\n",
    "    .mark_circle(size=100, opacity=0.9, stroke=None, strokeWidth=0)\n",
    "    .encode(\n",
    "        x=alt.X(\"feat1_woe:Q\", title=f\"{feat1} WOE\"),\n",
    "        y=alt.Y(\"feat2_woe:Q\", title=f\"{feat2} WOE\"),\n",
    "        color=alt.Color(\"predicted_prob:Q\", scale=alt.Scale(scheme=\"purples\", domain=[-2, 2]), title=\"WOE\"),\n",
    "        tooltip=[\n",
    "            alt.Tooltip(\"feat1_cat:N\", title=f\"{feat1} Category\"),\n",
    "            alt.Tooltip(\"feat2_cat:N\", title=f\"{feat2} Category\"),\n",
    "            alt.Tooltip(\"feat1_woe:Q\", title=f\"{feat1} WOE\", format=\".3f\"),\n",
    "            alt.Tooltip(\"feat2_woe:Q\", title=f\"{feat2} WOE\", format=\".3f\"),\n",
    "            alt.Tooltip(\"predicted_prob:Q\", title=\"Predicted Prob\", format=\".3f\"),\n",
    "            alt.Tooltip(\"target:N\", title=\"Actual Target\"),\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Text labels for categories\n",
    "text_labels = (\n",
    "    alt.Chart(unique_plot_data)\n",
    "    .mark_text(\n",
    "        align=\"center\",\n",
    "        baseline=\"middle\",\n",
    "        dx=0,\n",
    "        dy=-20,\n",
    "        fontSize=10,\n",
    "        fontWeight=\"bold\",\n",
    "    )\n",
    "    .encode(\n",
    "        x=alt.X(\"feat1_woe:Q\"),\n",
    "        y=alt.Y(\"feat2_woe:Q\"),\n",
    "        text=alt.Text(\"combined_label:N\"),\n",
    "        color=alt.value(\"black\"),  # text color\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine all plots\n",
    "combined_chart = (\n",
    "    (contour + scatter + text_labels)\n",
    "    .resolve_scale(color=\"shared\")\n",
    "    .properties(width=500, height=400, title=f\"{encoder}: Decision Surface - {feat1} vs {feat2}\")\n",
    ")\n",
    "\n",
    "mo.ui.altair_chart(combined_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Show WOE mappings and unique category combinations\n",
    "mapping1 = scorer_.get_mapping(feat1)\n",
    "mapping2 = scorer_.get_mapping(feat2)\n",
    "\n",
    "# Show unique combinations and their predicted probabilities\n",
    "unique_combinations = plot_data[['feat1_cat', 'feat2_cat', 'feat1_woe', 'feat2_woe', 'predicted_prob']].drop_duplicates()\n",
    "unique_combinations = unique_combinations.sort_values('predicted_prob', ascending=False)\n",
    "\n",
    "mo.md(\"**Unique Category Combinations - Ranked by Predicted Probability**\")\n",
    "mo.ui.table(unique_combinations.round(4))\n",
    "\n",
    "mo.hstack([\n",
    "    mo.vstack([\n",
    "        mo.md(f\"**{feat1} WOE Mapping ({encoder})**\"),\n",
    "        mo.ui.table(mapping1.round(4))\n",
    "    ]),\n",
    "    mo.vstack([\n",
    "        mo.md(f\"**{feat2} WOE Mapping ({encoder})**\"), \n",
    "        mo.ui.table(mapping2.round(4))\n",
    "    ])\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
