{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Boosting\n",
    "## Classification and regression\n",
    "\n",
    "This notebook demonstrates how to use the `RFGBoost` to perform gradient boosting with random forests as base learners.\n",
    "\n",
    "Author: https://www.github.com/deburky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmap import Colormap\n",
    "\n",
    "cmap1 = \"colorcet:cet_l19\"\n",
    "cmap2 = \"chrisluts:ygc_3c\"\n",
    "cm1 = Colormap(cmap1).to_mpl()\n",
    "cm2 = Colormap(cmap2).to_mpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression\n",
      "RFGBoost MSE: 6009.7267\n",
      "HistGradientBoosting MSE: 6667.1591\n",
      "\n",
      "Classification\n",
      "RFGBoost Log Loss: 0.2989\n",
      "HistGradientBoosting Log Loss: 0.3254\n"
     ]
    }
   ],
   "source": [
    "from rfgboost import RFGBoost\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000, n_features=10, noise=0.1, random_state=42\n",
    ")\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, random_state=42\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Random Forest Boost for Regression\n",
    "rf_boost_reg = RFGBoost(\n",
    "    n_estimators=10,\n",
    "    rf_params={\"n_estimators\": 50, \"max_depth\": 5},\n",
    "    task=\"regression\",\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "rf_boost_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = rf_boost_reg.predict(X_test_reg)\n",
    "\n",
    "# Random Forest Boost for Classification\n",
    "rf_boost_class = RFGBoost(\n",
    "    n_estimators=10,\n",
    "    rf_params={\"n_estimators\": 50, \"max_depth\": 5},\n",
    "    task=\"classification\",\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "rf_boost_class.fit(X_train_class, y_train_class)\n",
    "y_pred_class = rf_boost_class.predict(X_test_class)\n",
    "\n",
    "# HistGradientBoosting for Comparison\n",
    "hgb_reg = HistGradientBoostingRegressor(\n",
    "    max_iter=10, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "hgb_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_hgb_reg = hgb_reg.predict(X_test_reg)\n",
    "\n",
    "hgb_class = HistGradientBoostingClassifier(\n",
    "    max_iter=10, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "hgb_class.fit(X_train_class, y_train_class)\n",
    "y_pred_hgb_class = hgb_class.predict_proba(X_test_class)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "mse_rf_boost_reg = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "mse_hgb_reg = mean_squared_error(y_test_reg, y_pred_hgb_reg)\n",
    "\n",
    "logloss_rf_boost_class = log_loss(y_test_class, y_pred_class)\n",
    "logloss_hgb_class = log_loss(y_test_class, y_pred_hgb_class)\n",
    "\n",
    "print(\n",
    "    f\"Regression\\nRFGBoost MSE: {mse_rf_boost_reg:.4f}\\nHistGradientBoosting MSE: {mse_hgb_reg:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nClassification\\nRFGBoost Log Loss: {logloss_rf_boost_class:.4f}\\nHistGradientBoosting Log Loss: {logloss_hgb_class:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n",
      "Regression MSE: 7140.9811\n",
      "Classification Log Loss: 0.4020\n",
      "\n",
      "XGBoost\n",
      "Regression MSE: 6229.6563\n",
      "Classification Log Loss: 0.3143\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "# CatBoost for Regression\n",
    "cb_reg = cb.CatBoostRegressor(\n",
    "    iterations=10,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "cb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# CatBoost for Classification\n",
    "cb_class = cb.CatBoostClassifier(\n",
    "    iterations=10,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    allow_writing_files=False,\n",
    ")\n",
    "cb_class.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_cb_reg = cb_reg.predict(X_test_reg)\n",
    "mse_cb_reg = mean_squared_error(y_test_reg, y_pred_cb_reg)\n",
    "\n",
    "y_pred_cb_class = cb_class.predict_proba(X_test_class)[:, 1]\n",
    "\n",
    "logloss_cb_class = log_loss(y_test_class, y_pred_cb_class)\n",
    "\n",
    "print(\n",
    "    f\"CatBoost\\nRegression MSE: {mse_cb_reg:.4f}\\nClassification Log Loss: {logloss_cb_class:.4f}\"\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# XGBoost for Regression\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=10, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "xgb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# XGBoost for Classification\n",
    "xgb_class = xgb.XGBClassifier(\n",
    "    n_estimators=10, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "xgb_class.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_xgb_reg = xgb_reg.predict(X_test_reg)\n",
    "mse_xgb_reg = mean_squared_error(y_test_reg, y_pred_xgb_reg)\n",
    "\n",
    "y_pred_xgb_class = xgb_class.predict_proba(X_test_class)[:, 1]\n",
    "\n",
    "logloss_xgb_class = log_loss(y_test_class, y_pred_xgb_class)\n",
    "\n",
    "print(\n",
    "    f\"\\nXGBoost\\nRegression MSE: {mse_xgb_reg:.4f}\\nClassification Log Loss: {logloss_xgb_class:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and classification visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams.update({\"font.size\": 13})\n",
    "\n",
    "# Create a 2D regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=2, noise=0.5, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "x_min, x_max = X_reg[:, 0].min() - 1, X_reg[:, 0].max() + 1\n",
    "y_min, y_max = X_reg[:, 1].min() - 1, X_reg[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Models and parameters\n",
    "n_iterations = 50  # Reduced iterations for faster computation\n",
    "rf_boost_reg = RFGBoost(\n",
    "    n_estimators=n_iterations,\n",
    "    rf_params={\"n_estimators\": 20, \"max_depth\": 5},\n",
    "    task=\"regression\",\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "hgb_reg = HistGradientBoostingRegressor(\n",
    "    max_iter=n_iterations, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "cb_reg = None  # Initialize empty CatBoost model\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=n_iterations, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "\n",
    "models = [\n",
    "    (\"RFGBoost\", rf_boost_reg),\n",
    "    (\"HistGradientBoosting\", hgb_reg),\n",
    "    (\"CatBoost\", None),  # Placeholder for CatBoost\n",
    "    (\"XGBoost\", xgb_reg),\n",
    "]\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Generate decision boundaries for all models across iterations\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8), dpi=120)  # 2x2 grid\n",
    "    # fig.set_size_inches(12, 10)  # Slightly larger canvas\n",
    "    fig.subplots_adjust(\n",
    "        top=0.85,  # move suptitle down\n",
    "        bottom=0.08,  # move subplot grid down a little\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        wspace=0.3,\n",
    "        hspace=0.3,\n",
    "    )\n",
    "    axs = axs.ravel()  # Flatten axes for easier access\n",
    "\n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        if name == \"CatBoost\":\n",
    "            if iteration == 1:\n",
    "                cb_reg = cb.CatBoostRegressor(\n",
    "                    iterations=1,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=5,\n",
    "                    verbose=0,\n",
    "                    random_state=42,\n",
    "                    allow_writing_files=False,\n",
    "                )\n",
    "                cb_reg.fit(X_train_reg, y_train_reg)\n",
    "            else:\n",
    "                cb_reg.fit(X_train_reg, y_train_reg, init_model=cb_reg)\n",
    "            model = cb_reg\n",
    "\n",
    "        elif iteration == 1:\n",
    "            model.fit(X_train_reg, y_train_reg)  # Fit full model initially\n",
    "\n",
    "        # Adjust training for models that require iterative updates\n",
    "        if name == \"HistGradientBoosting\":\n",
    "            model.set_params(max_iter=iteration)\n",
    "            model.fit(X_train_reg, y_train_reg)\n",
    "        elif name == \"XGBoost\":\n",
    "            model.set_params(n_estimators=iteration)\n",
    "            model.fit(X_train_reg, y_train_reg)\n",
    "        elif name == \"RFGBoost\":\n",
    "            model = RFGBoost(\n",
    "                n_estimators=iteration,\n",
    "                rf_params={\"n_estimators\": 20, \"max_depth\": 5},\n",
    "                task=\"regression\",\n",
    "                learning_rate=0.1,\n",
    "            )\n",
    "            model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "        # Predict decision boundary\n",
    "        preds = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "        # Plot on the current subplot\n",
    "        axs[idx].contourf(xx, yy, preds, alpha=1.0, levels=10, cmap=cm1)\n",
    "        axs[idx].scatter(\n",
    "            X_train_reg[:, 0],\n",
    "            X_train_reg[:, 1],\n",
    "            c=y_train_reg,\n",
    "            edgecolor=\"k\",\n",
    "            cmap=cm1,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "        axs[idx].set_title(f\"{name}\")\n",
    "        fig.suptitle(\n",
    "            f\"Regression (Boosting Iteration {iteration})\", fontsize=25, y=0.96\n",
    "        )\n",
    "\n",
    "        # Add MSE to the plot\n",
    "        mse = mean_squared_error(y_test_reg, model.predict(X_test_reg))\n",
    "        axs[idx].text(\n",
    "            0.02, 0.03, f\"MSE: {mse:.4f}\", transform=axs[idx].transAxes, fontsize=12\n",
    "        )\n",
    "        axs[idx].set_xlim(x_min, x_max)\n",
    "        axs[idx].set_ylim(y_min, y_max)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # Save frame for GIF\n",
    "    fig.canvas.draw()\n",
    "    image = np.array(fig.canvas.buffer_rgba()).reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (4,)\n",
    "    )[:, :, :3]  # Convert RGBA to RGB\n",
    "    frames.append(image)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save the animation as a GIF\n",
    "iio.imwrite(\"regression_animation_grid.gif\", frames, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams.update({\"font.size\": 13})\n",
    "\n",
    "X = np.random.rand(500, 2) * 6 - 3  # Random points in the range [-3, 3]\n",
    "y = (X[:, 1] > np.sin(X[:, 0])).astype(int)\n",
    "X_class, y_class = X, y\n",
    "\n",
    "# Correctly splitting the XOR data\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create meshgrid for decision boundary\n",
    "x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1\n",
    "y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
    "\n",
    "# Models and parameters\n",
    "n_iterations = 50  # Reduced iterations for efficiency\n",
    "rf_boost_class = RFGBoost(\n",
    "    n_estimators=n_iterations,\n",
    "    rf_params={\"n_estimators\": 20, \"max_depth\": 5},\n",
    "    task=\"classification\",\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "hgb_class = HistGradientBoostingClassifier(\n",
    "    max_iter=n_iterations, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "cb_class = None  # Initialize empty CatBoost model\n",
    "xgb_class = xgb.XGBClassifier(\n",
    "    n_estimators=n_iterations, learning_rate=0.1, max_depth=5, random_state=42\n",
    ")\n",
    "\n",
    "models = [\n",
    "    (\"RFGBoost\", rf_boost_class),\n",
    "    (\"HistGradientBoosting\", hgb_class),\n",
    "    (\"CatBoost\", None),  # Placeholder for CatBoost\n",
    "    (\"XGBoost\", xgb_class),\n",
    "]\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Generate decision boundaries for all models across iterations\n",
    "for iteration in range(1, n_iterations + 1):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8), dpi=120)  # 2x2 grid\n",
    "    # fig.set_size_inches(12, 10)  # Slightly larger canvas\n",
    "    fig.subplots_adjust(\n",
    "        top=0.85,  # move suptitle down\n",
    "        bottom=0.08,  # move subplot grid down a little\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        wspace=0.3,\n",
    "        hspace=0.3,\n",
    "    )\n",
    "    axs = axs.ravel()  # Flatten axes for easier access\n",
    "\n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        if name == \"CatBoost\":\n",
    "            if iteration == 1:\n",
    "                cb_class = cb.CatBoostClassifier(\n",
    "                    iterations=1,\n",
    "                    learning_rate=0.1,\n",
    "                    depth=5,\n",
    "                    verbose=0,\n",
    "                    random_state=42,\n",
    "                    allow_writing_files=False,\n",
    "                )\n",
    "                cb_class.fit(X_train_class, y_train_class)\n",
    "            else:\n",
    "                cb_class.fit(X_train_class, y_train_class, init_model=cb_class)\n",
    "            model = cb_class\n",
    "\n",
    "        elif iteration == 1:\n",
    "            model.fit(X_train_class, y_train_class)  # Fit full model initially\n",
    "\n",
    "        # Adjust training for models that require iterative updates\n",
    "        if name == \"HistGradientBoosting\":\n",
    "            model.set_params(max_iter=iteration)\n",
    "            model.fit(X_train_class, y_train_class)\n",
    "        elif name == \"XGBoost\":\n",
    "            model.set_params(n_estimators=iteration)\n",
    "            model.fit(X_train_class, y_train_class)\n",
    "        elif name == \"RFGBoost\":\n",
    "            model = RFGBoost(\n",
    "                n_estimators=iteration,\n",
    "                rf_params={\"n_estimators\": 20, \"max_depth\": 5},\n",
    "                task=\"classification\",\n",
    "                learning_rate=0.1,\n",
    "            )\n",
    "            model.fit(X_train_class, y_train_class)\n",
    "\n",
    "        # Predict decision boundary (classification probability for class 1)\n",
    "        if name == \"RFGBoost\":\n",
    "            preds = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        else:\n",
    "            preds = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(\n",
    "                xx.shape\n",
    "            )\n",
    "\n",
    "        # Plot on the current subplot\n",
    "        axs[idx].contourf(xx, yy, preds, alpha=0.5, levels=10, cmap=cm2)\n",
    "        axs[idx].scatter(\n",
    "            X_train_class[:, 0],\n",
    "            X_train_class[:, 1],\n",
    "            c=y_train_class,\n",
    "            edgecolor=\"k\",\n",
    "            cmap=cm2,\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        axs[idx].set_title(f\"{name}\")\n",
    "        fig.suptitle(\n",
    "            f\"Classification (Boosting Iteration {iteration})\", fontsize=25, y=0.96\n",
    "        )\n",
    "\n",
    "        # Add Log Loss to the plot\n",
    "        if name == \"RFGBoost\":\n",
    "            logloss = log_loss(y_test_class, model.predict(X_test_class))\n",
    "        else:\n",
    "            logloss = log_loss(y_test_class, model.predict_proba(X_test_class)[:, 1])\n",
    "        axs[idx].text(\n",
    "            0.02,\n",
    "            0.03,\n",
    "            f\"Log Loss: {logloss:.4f}\",\n",
    "            transform=axs[idx].transAxes,\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axs[idx].set_xlim(x_min, x_max)\n",
    "        axs[idx].set_ylim(y_min, y_max)\n",
    "\n",
    "    # Save frame for GIF\n",
    "    fig.canvas.draw()\n",
    "    image = np.array(fig.canvas.buffer_rgba()).reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (4,)\n",
    "    )[:, :, :3]  # Convert RGBA to RGB\n",
    "    frames.append(image)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save the animation as a GIF\n",
    "iio.imwrite(\"classification_animation_grid.gif\", frames, fps=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
